# Gates Dupont #
# 2019         #
################

#----Libraries----
library(rgdal)
library(proj4)
library(sp)
library(dplyr)
library(mgcv) 
library(ggplot2)
library(raster)
library(stringr)
library(rgeos)

#----Species input for files----
species = "sonspa"

#----Setup----
setwd("~/WNV SP19")

#----Importing PFW data----
if(T){
  raw.pfw = read.csv(paste0("PFW_",species,".csv"))
  # rm(list=setdiff(ls(), "raw.pfw"))
}

#----Converting to duplicate data for preservation----
rawData = raw.pfw

#----Importing geocode locations----
ref.locs = read.csv("pfwfinal00.csv")
for(i in 1:8){
  ref.locs = rbind(ref.locs, read.csv(paste0("pfwfinal0",as.character(i),".csv")))
} # 1+8 files

#----Removing postal code IDs----
ref.locs = droplevels(ref.locs[!(ref.locs$LOC_METHOD == "postal"),])

#----Selecting states----
states = c('ME', 'NH', 'VT', 'MA', 'RI', 'CT', 'NY', 'PA', 'NJ')
rawData = rawData[rawData$StatProv %in% states,]

#----Preparing to merge with geocoded locs----
rawData$season = as.numeric(str_sub(rawData$FW_Year, start= -4))
rawData$lat = NA; rawData$long = NA

#----Merging geocoded locations----
geo.pfw = merge.data.frame(rawData, ref.locs, by = "ID", all.x = T)

#----Keeping and combining good coords----

# Removing zipcode-only locs
geo.pfw = geo.pfw[!((str_detect(str_to_lower(geo.pfw$ENTRY_TECHNIQUE), "post") == TRUE) &is.na(geo.pfw$LATITUDE.y)),]

# Adding good locs to new lat/long columns
geo.pfw$LATITUDE.y[is.na(geo.pfw$LATITUDE.y)] = geo.pfw$LATITUDE.x[is.na(geo.pfw$LATITUDE.y)]
geo.pfw$LONGITUDE.y[is.na(geo.pfw$LONGITUDE.y)] = geo.pfw$LONGITUDE.x[is.na(geo.pfw$LONGITUDE.y)]

#----Formatting effort----
rawData = geo.pfw
rawData[rawData==""] = NA # blank to NA
rawData = rawData %>%
  filter(complete.cases(.[,c(1, 43:44, 36, 28:29, 25, 11:15)])) %>% # remove NAs 1, 36, 25, 43:44, 15, 29
  mutate(effortDaysNumerical = rowSums(.[,11:14])) %>% # Summing half days
  mutate(obshrs=
           recode(EFFORT_HRS_ATLEAST,
                  "0.001"=1,"1"=2,"4"=3,"4.001"=3,"8.001"=4)) %>% # recoding effort
  dplyr::select(lat=43, long=44, yr=36, day=28, maxFlock=25,
                effortHours=48, effortDays=47,locID=1) %>% # selecting relevant data
  droplevels() # check this

dfEffHD=droplevels(rawData)

#----Keeping only points from within region of interest----

# Generating polygon
states.full = c("Maine", "New Hampshire", "Vermont", "Massachusetts",
                "Rhode Island", "Connecticut", "New York", "Pennsylvania",
                "New Jersey")

# Load state polygons
us = raster::getData('GADM', country = 'US', level = 1) # get admin. boundaries
st.contour = us[us$NAME_1 %in% states.full,] # keep only predefined states

# Extending buffer to preserve coastal locs
st.contour = spTransform(x=st.contour, CRSobj=CRS("+init=epsg:32662"))
st.contour.buffer = gBuffer(st.contour, width = 10000) 
st.contour = spTransform(st.contour.buffer, CRSobj=CRS("+init=epsg:4326"))

# Converting pfw data to spcoordinates(dfEffHD)= ~long+lat # converting from f to spdf
crs(dfEffHD) = CRS("+init=epsg:4326") # adding crs format

# Selecting only points in the polygon (this takes a while)
dfEffHD = dfEffHD[st.contour,] 
dfEffHD = as.data.frame(dfEffHD) # convert back to df from spdf

#----Selecting power users----
yearly_list_count = table(dfEffHD$locID, dfEffHD$yr) # creates a table showing number of observations at each location each year.
row_sums = rowSums(yearly_list_count >= 10) # rows where there are at least 10 observations
threeyears = which(row_sums >=3) # for rows with 10 obs over at least 3 years
newIDs = names(threeyears) # just setting a new variable name
# Final product:
df = dfEffHD[dfEffHD$locID %in% newIDs,] 

#----Exporting file----
write.csv(df, paste0("PFW_", species, "_clean_geocode.csv"))
